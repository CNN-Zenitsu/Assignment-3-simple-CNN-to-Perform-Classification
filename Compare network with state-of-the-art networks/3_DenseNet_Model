import tensorflow as tf
from tensorflow.keras import layers, models
# Import only DenseNet121
from tensorflow.keras.applications import DenseNet121
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support
import seaborn as sns
import pandas as pd

# Load pre-trained DenseNet121 without its top classification layer
base_model_densenet = DenseNet121(
    weights='imagenet',
    include_top=False,
    input_shape=(img_height, img_width, 3)
)

# Freeze the base model initially so we only train our new layers
base_model_densenet.trainable = False

# Build our new model by adding a custom "head" to the DenseNet base
model_densenet = models.Sequential([
    base_model_densenet,
    layers.GlobalAveragePooling2D(),
    layers.BatchNormalization(),
    layers.Dense(512, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(256, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(num_classes, activation='softmax') # The final output layer
], name='DenseNet121_Transfer')

# Compile the model for the first phase of training
model_densenet.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

print("\nHere is a summary of the DenseNet121 model's architecture:")
model_densenet.summary()


# These callbacks will help optimize the training process
callbacks_densenet = [
    # Stop training early if the validation loss stops improving
    tf.keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=10,
        restore_best_weights=True,
        verbose=1
    ),
    # Reduce the learning rate if training plateaus
    tf.keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=5,
        min_lr=1e-7,
        verbose=1
    ),
    # Save the best version of the model based on validation accuracy
    tf.keras.callbacks.ModelCheckpoint(
        'densenet121_best.h5',
        monitor='val_accuracy',
        save_best_only=True,
        verbose=1
    )
]


print("\n" + "=" * 80)
print("Starting Phase 1: Training the new top layers (base model is frozen).")
print("=" * 80)

print("\nTraining DenseNet121 (Phase 1)...")
history_densenet_phase1 = model_densenet.fit(
    train_ds,
    validation_data=val_ds,
    epochs=20,
    callbacks=callbacks_densenet,
    verbose=1
)


print("\n" + "=" * 80)
print("Starting Phase 2: Fine-tuning the model (unfreezing some base layers).")
print("=" * 80)

# Unfreeze the last 30 layers of the base model
base_model_densenet.trainable = True
for layer in base_model_densenet.layers[:-30]:
    layer.trainable = False

# Re-compile the model with a much lower learning rate for fine-tuning
model_densenet.compile(
    optimizer=Adam(learning_rate=0.0001),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

trainable_layer_count = len([l for l in base_model_densenet.layers if l.trainable])
print(f"\nDenseNet121 now has {trainable_layer_count} trainable layers for fine-tuning.")

print("\nFine-tuning DenseNet121 (Phase 2)...")
# Continue training from where we left off
history_densenet_phase2 = model_densenet.fit(
    train_ds,
    validation_data=val_ds,
    epochs=30,
    initial_epoch=len(history_densenet_phase1.history['loss']),
    callbacks=callbacks_densenet,
    verbose=1
)


# Combine the histories from both phases for plotting
history_densenet = {
    'accuracy': history_densenet_phase1.history['accuracy'] + history_densenet_phase2.history['accuracy'],
    'val_accuracy': history_densenet_phase1.history['val_accuracy'] + history_densenet_phase2.history['val_accuracy'],
    'loss': history_densenet_phase1.history['loss'] + history_densenet_phase2.history['loss'],
    'val_loss': history_densenet_phase1.history['val_loss'] + history_densenet_phase2.history['val_loss']
}


print("\n" + "=" * 80)
print("Step 2: Evaluating the final model on the test dataset.")
print("=" * 80)

print("\nEvaluating DenseNet121:")
test_loss_densenet, test_acc_densenet = model_densenet.evaluate(test_ds)
print(f"Test Accuracy: {test_acc_densenet*100:.2f}%")
print(f"Test Loss: {test_loss_densenet:.4f}")


print("\n" + "=" * 80)
print("Step 3: Plotting the training and validation history.")
print("=" * 80)

print("\nNow generating plots for accuracy and loss...")

# Create a figure with two subplots (one for accuracy, one for loss)
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# Plot DenseNet121 Accuracy
axes[0].plot(history_densenet['accuracy'], label='Train', linewidth=2)
axes[0].plot(history_densenet['val_accuracy'], label='Val', linewidth=2)
axes[0].axvline(x=len(history_densenet_phase1.history['accuracy']),
                   color='r', linestyle='--', label='Fine-tuning starts')
axes[0].set_title('DenseNet121 - Accuracy', fontsize=14, fontweight='bold')
axes[0].set_xlabel('Epoch')
axes[0].set_ylabel('Accuracy')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Plot DenseNet121 Loss
axes[1].plot(history_densenet['loss'], label='Train', linewidth=2)
axes[1].plot(history_densenet['val_loss'], label='Val', linewidth=2)
axes[1].axvline(x=len(history_densenet_phase1.history['loss']),
                   color='r', linestyle='--', label='Fine-tuning starts')
axes[1].set_title('DenseNet121 - Loss', fontsize=14, fontweight='bold')
axes[1].set_xlabel('Epoch')
axes[1].set_ylabel('Loss')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('training_history_densenet.png', dpi=300, bbox_inches='tight')
plt.show()


print("\n" + "=" * 80)
print("Step 4: Generating predictions for the test set.")
print("=" * 80)

print("\nUsing the trained model to make predictions...")

y_true = []
y_pred_densenet = []

# Iterate over the test dataset to get predictions and true labels
for images, labels in test_ds:
    y_true.extend(labels.numpy())

    # Get model predictions
    pred_densenet = model_densenet.predict(images, verbose=0)
    y_pred_densenet.extend(np.argmax(pred_densenet, axis=1))


print("\n" + "=" * 80)
print("Step 5: Generating the confusion matrix.")
print("=" * 80)

print("\nCreating the confusion matrix plot...")

plt.figure(figsize=(10, 8))

# Create the confusion matrix
cm_densenet = confusion_matrix(y_true, y_pred_densenet)

# Display it as a heatmap
ax = sns.heatmap(cm_densenet, annot=True, fmt='d', cmap='Purples',
            xticklabels=class_names, yticklabels=class_names)
ax.set_title(f'DenseNet121\nAccuracy: {test_acc_densenet*100:.2f}%',
                  fontsize=14, fontweight='bold')
ax.set_ylabel('True Label')
ax.set_xlabel('Predicted Label')

plt.tight_layout()
plt.savefig('confusion_matrix_densenet.png', dpi=300, bbox_inches='tight')
plt.show()


print("\n" + "=" * 80)
print("Step 6: Displaying the Classification Report.")
print("=" * 80)

print("\nDetailed Classification Report for DenseNet121:")
print(classification_report(y_true, y_pred_densenet, target_names=class_names))


print("\n" + "=" * 80)
print("Step 7: Compiling the Final Performance Summary.")
print("=" * 80)

# Calculate weighted-average metrics for the report
precision_densenet, recall_densenet, f1_densenet, _ = precision_recall_fscore_support(
    y_true, y_pred_densenet, average='weighted'
)

# Create a DataFrame for a clean summary table
comparison_df = pd.DataFrame({
    'Model': ['DenseNet121'],
    'Test Accuracy (%)': [test_acc_densenet*100],
    'Test Loss': [test_loss_densenet],
    'Precision': [precision_densenet],
    'Recall': [recall_densenet],
    'F1-Score': [f1_densenet],
    'Parameters': [model_densenet.count_params()]
})

# Format the numbers for better readability
comparison_df['Test Accuracy (%)'] = comparison_df['Test Accuracy (%)'].map('{:,.2f}%'.format)
comparison_df['Parameters'] = comparison_df['Parameters'].map('{:,}'.format)
comparison_df['Test Loss'] = comparison_df['Test Loss'].map('{:.4f}'.format)
comparison_df['Precision'] = comparison_df['Precision'].map('{:.4f}'.format)
comparison_df['Recall'] = comparison_df['Recall'].map('{:.4f}'.format)
comparison_df['F1-Score'] = comparison_df['F1-Score'].map('{:.4f}'.format)

print("\n" + comparison_df.to_string(index=False))


print("\n" + "=" * 80)
print("Step 8: Analyzing Model Size and Parameters.")
print("=" * 80)

# Create a DataFrame to show parameter details
efficiency_df = pd.DataFrame({
    'Model': ['DenseNet121'],
    'Total Parameters': [
        f"{model_densenet.count_params():,}"
    ],
    'Trainable Parameters': [
        f"{sum([tf.keras.backend.count_params(w) for w in model_densenet.trainable_weights]):,}"
    ],
    'Model Size (MB)': [
        f"{model_densenet.count_params() * 4 / (1024**2):.2f}"
    ]
})

print("\n" + efficiency_df.to_string(index=False))


print("\nSaving the final trained model to disk...")
model_densenet.save('densenet121_transfer_learning_final.h5')
print("The model has been saved successfully as 'densenet121_transfer_learning_final.h5'")


print("\n" + "=" * 80)
print("All done! The DenseNet121 model has been trained and evaluated.")
print("=" * 80)
print("\nThe following files were created during this process:")
print("  - densenet121_best.h5 (The best checkpoint from training)")
print("  - densenet121_transfer_learning_final.h5 (The final saved model)")
print("  - training_history_densenet.png (Accuracy and loss plots)")
print("  - confusion_matrix_densenet.png (The final confusion matrix)")